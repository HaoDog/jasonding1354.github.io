
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  
    <title>【机器学习基础】梯度提升决策树 | Jason&#39;s Techblog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="Jason Ding">
    
    <meta name="description" content="引言上一节中介绍了《随机森林算法》，该算法使用bagging的方式作出一些决策树来，同时在决策树的学习过程中加入了更多的随机因素。该模型可以自动做到验证过程同时还可以进行特征选择。这一节，我们将决策树和AdaBoost算法结合起来，在AdaBoost中每一轮迭代，都会给数据更新一个权重，利用这个权重">
    
    
    
    
    <link rel="alternate" href="/atom.xml" title="Jason&#39;s Techblog" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/author.png">
    <link rel="apple-touch-icon-precomposed" href="/img/author.png">
    
    <link rel="stylesheet" href="/css/style.css">
    
<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F3cc71cd5738b99a1db174951e194ba55' type='text/javascript'%3E%3C/script%3E"));


var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F1dcf1fa58b0eca1ac5bd089ee6a6e78c' type='text/javascript'%3E%3C/script%3E"));


</script>


    <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>


  <body>
    <header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="Jason&#39;s Techblog" title="Jason&#39;s Techblog"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Jason&#39;s Techblog">Jason&#39;s Techblog</a></h1>
				<h2 class="blog-motto">Technician =&gt; Scientist =&gt; Philosopher =&gt; Artists</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="http://jasonding.top">Main</a></li>
					
					<li>
					
						<form class="search" action="/search/index.html" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="text" id="search" class="st-default-search-input" maxlength="20" placeholder="Search" />
						</form>
					
					</li>
				</ul>
			</nav>			
</div>

    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody">
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2015/07/24/Machine Learning/【机器学习基础】梯度提升决策树/" title="【机器学习基础】梯度提升决策树" itemprop="url">【机器学习基础】梯度提升决策树</a>
  </h1>
  <p class="article-author">By
    
      <a href="http://blog.jasonding.top" title="Jason Ding">Jason Ding</a>
    </p>
  <p class="article-time">
    <time datetime="2015-07-24T11:53:44.000Z" itemprop="datePublished">2015-07-24</time>
    Updated:<time datetime="2016-03-16T06:24:42.000Z" itemprop="dateModified">2016-03-16</time>
    
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#引言"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-加权的决策树算法-Weighted-Decision-Tree-Algorithm"><span class="toc-number">2.</span> <span class="toc-text">1. 加权的决策树算法(Weighted Decision Tree Algorithm)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-弱决策树算法"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 弱决策树算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-AdaBoost-Stump"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 AdaBoost-Stump</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-AdaBoost深入解释和最佳化"><span class="toc-number">3.</span> <span class="toc-text">2. AdaBoost深入解释和最佳化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-投票分数-Voting-Score-和间隔-Margin-的关系"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 投票分数(Voting Score)和间隔(Margin)的关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-AdaBoost误差函数"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 AdaBoost误差函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-AdaBoost误差函数的梯度下降求解"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 AdaBoost误差函数的梯度下降求解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-最佳化步长η"><span class="toc-number">3.4.</span> <span class="toc-text">2.4 最佳化步长η</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-小结"><span class="toc-number">3.5.</span> <span class="toc-text">2.5 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-梯度提升-Gradient-Boosting"><span class="toc-number">4.</span> <span class="toc-text">3. 梯度提升(Gradient Boosting)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-梯度提升应用于回归问题"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 梯度提升应用于回归问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-梯度提升决策树"><span class="toc-number">5.</span> <span class="toc-text">4. 梯度提升决策树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考资料"><span class="toc-number">6.</span> <span class="toc-text">参考资料</span></a></li></ol>
		</div>
		
		<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>上一节中介绍了<a href="http://jasonding1354.github.io/2015/07/23/Machine%20Learning/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95/" target="_blank" rel="external">《随机森林算法》</a>，该算法使用bagging的方式作出一些决策树来，同时在决策树的学习过程中加入了更多的随机因素。该模型可以自动做到验证过程同时还可以进行特征选择。<br>这一节，我们将决策树和AdaBoost算法结合起来，在AdaBoost中每一轮迭代，都会给数据更新一个权重，利用这个权重，我们学习得到一个g，在这里我们得到一个决策树，最终利用线性组合的方式得到多个决策树组成的G。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/abdt1.jpg" alt=""></p>
<h2 id="1-加权的决策树算法-Weighted-Decision-Tree-Algorithm"><a href="#1-加权的决策树算法-Weighted-Decision-Tree-Algorithm" class="headerlink" title="1. 加权的决策树算法(Weighted Decision Tree Algorithm)"></a>1. 加权的决策树算法(Weighted Decision Tree Algorithm)</h2><p>在引言中介绍了，我们利用AdaBoost的思想，为数据赋予不同的权重，而在将要介绍的Adaptive Boosted Decision Tree中，要建立加权的决策树，所以接下来就要介绍它。<br>在之前含有权重的算法中，权重作为衡量数据重要性的一项指标，用来评估训练误差，如下面的公式所示：<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/abdt2.jpg" alt=""></p>
<p>上面的公式中，权重是包含在误差的公式中的，也就是说，我们想要构造一个带有权重的决策树算法，需要改造决策树的误差度量函数。然而换一个角度，权重也可以等效于数据的重复次数，重复次数越多的数据，说明其越重要。在广义的随机算法中，我们可以根据权重比例来进行随机采样（比如，如果权重比例是30%，那么采样该数据的概率就是30%），根据这种方式得到一组新的数据，那么这组新的数据中的比例大概就是和权重的比例呈正比的，也就是说它能够表达权重对于数据的意义。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/abdt3.jpg" alt=""><br>在AdaBoost-DTree中，为了简单起见，我们不去改变AdaBoost的框架，也不去修改决策树的内部细节，而只是通过基于权重的训练数据的采样来实现。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/abdt4.jpg" alt=""></p>
<h3 id="1-1-弱决策树算法"><a href="#1-1-弱决策树算法" class="headerlink" title="1.1 弱决策树算法"></a>1.1 弱决策树算法</h3><p>在AdaBoost算法中，我们通过错误率<code>εt</code>来计算单个的g的权重<code>αt</code>，那么如果我们使用决策树作为g的时候，g是一个完全长成的树，该树对整个数据集进行细致的切分导致<code>Ein=0</code>，那么这使得<code>εt=0</code>，但计算得到的权重<code>αt</code>会变成无限大。<br>其意义是，如果使用一个能力很强的树作为g的话，那么该算法会赋予该树无限大的权重或票数，最终得到了一棵“独裁”的树（因为AdaBoost的哲学意义是庶民政治，就是集中多方的意见，及时有的意见可能是错误的），违背了AdaBoost的宗旨。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/abdt5.jpg" alt=""></p>
<p>上面的问题出在使用了所有的数据和让树完全长成这两方面。针对这两个问题，我们要通过剪枝和部分训练数据得到一个弱一点的树。<br>所以实际上，AdaBoost-DTree是通过sampling的方式得到部分训练数据，通过剪枝的方式限制树的高度，得到弱一点的决策树。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/abdt6.jpg" alt=""></p>
<h3 id="1-2-AdaBoost-Stump"><a href="#1-2-AdaBoost-Stump" class="headerlink" title="1.2 AdaBoost-Stump"></a>1.2 AdaBoost-Stump</h3><p>什么样是树才是弱决策树呢？<br>我们这里限制这棵树只有一层，即决策桩(Decision Stump)。这样我们需要让CART树的不纯度(impurity)尽可能低，学习一个决策桩。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/abdt7.jpg" alt=""><br>所以，使用决策桩作为弱分类器的AdaBoost称为AdaBoost-Stump，它是一种特殊的AdaBoost-DTree。</p>
<h2 id="2-AdaBoost深入解释和最佳化"><a href="#2-AdaBoost深入解释和最佳化" class="headerlink" title="2. AdaBoost深入解释和最佳化"></a>2. AdaBoost深入解释和最佳化</h2><p>我们回顾一下AdaBoost算法流程：<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/adaboost/adaboost4.jpg" alt=""><br>其中权重un(t+1)通过<code>◆t</code>对un(t)进行修正得到，由于<code>◆t</code>是一个大于1的数，对错误数据加大权重以让算法更加重视错分的数据。<br>我们可以用下面的方式来描述un(t+1)：<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost1.jpg" alt=""></p>
<p>下面的公式是我们将un(T+1)展开，我们看到图中橘色的部分<code>∑αt·gt(xn)</code>是G(x)中的分数，它出现在AdaBoost的权重的表达式中，我们称<code>∑αt·gt(xn)</code>为投票分数(voting score)。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost2.jpg" alt=""></p>
<p>所以，AdaBoost里面每一个数据的权重，和<code>exp(-yn(voting score on xn))</code>呈正比。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost3.jpg" alt=""></p>
<h3 id="2-1-投票分数-Voting-Score-和间隔-Margin-的关系"><a href="#2-1-投票分数-Voting-Score-和间隔-Margin-的关系" class="headerlink" title="2.1 投票分数(Voting Score)和间隔(Margin)的关系"></a>2.1 投票分数(Voting Score)和间隔(Margin)的关系</h3><p>线性混合(linear blending)等价于将假设看做是特征转换的线性模型：<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost4.jpg" alt=""><br>其中<code>αt·gt(xn)</code>如果换作是<code>wT·φ(xn)</code>可能就更清楚了，这与下面给出的在SVM中的margin表达式对比，我们可以明白投票分数<code>∑αt·gt(xn)</code>的物理意义，即可以看做是没有正规化的边界(margin)。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost5.jpg" alt=""></p>
<p>所以，<code>yn(voting score)</code>是有符号的、没有正规化的边界距离，从这个角度来说，我们希望<code>yn(voting score)</code>越大越好，因为这样的泛化能力越强。于是，<code>exp(-yn(voting score))</code>越小越好，那么un(T+1)越小越好。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost6.jpg" alt=""><br>AdaBoost在迭代过程中，是让<code>∑un(t)</code>越来越小的过程，在这个过程中，逐渐达到SVM中最大分类间隔的效果。</p>
<h3 id="2-2-AdaBoost误差函数"><a href="#2-2-AdaBoost误差函数" class="headerlink" title="2.2 AdaBoost误差函数"></a>2.2 AdaBoost误差函数</h3><p>上面解释到了，AdaBoost在迭代学习的过程，就是希望让<code>∑un(t)</code>越来越小的过程，那么我们新的目标就是最佳化<code>∑un(T+1)</code>：<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost7.jpg" alt=""><br>我们可以画出0/1错误和AdaBoost误差函数<code>err(s,y) = exp(-ys)</code>的函数曲线，我们发现AdaBoost的误差函数（称为exponential error measure）实际上也是0/1错误函数的上限函数，于是，我们可以通过最小化该函数来起到最佳化的效果。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost8.jpg" alt=""></p>
<h3 id="2-3-AdaBoost误差函数的梯度下降求解"><a href="#2-3-AdaBoost误差函数的梯度下降求解" class="headerlink" title="2.3 AdaBoost误差函数的梯度下降求解"></a>2.3 AdaBoost误差函数的梯度下降求解</h3><p>为了最小化AdaBoost的误差函数Ein，我们可以将Ein函数在所在点的附近做泰勒展开，我们就可以发现在该点的附近可以被梯度所描述，我们希望求一个最好的方向（最大梯度相反的方向），然后在该方向上走一小步，这样我们就可以做到比现在的函数效果好一点点，依次进行梯度下降，最终达到最小化误差函数的效果。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost9.jpg" alt=""></p>
<p>现在我们把gt当做方向，希望去找到这个gt（这里函数方向gt和上面介绍的最大梯度的方向向量没有什么差别，只是表示方式有所不同而已）。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost10.jpg" alt=""><br>我们解释一下上面的公式：</p>
<blockquote>
<ul>
<li>我们需要找到一个新的函数，这里表示为h(xn)和步长η，将这个函数加入到表达式中去；</li>
<li>我们将第一个公式中紫色的部分合起来，简化表示为权重un(t)；</li>
<li>将<code>exp(-y·η·h(xn))</code>在原点处做泰勒展开，得到<code>(1-yn·η·h(xn))</code>；</li>
<li>然后拆成两部分<code>∑un(t)</code>和<code>η·∑un(t)·yn·h(xn)</code>，第一部分是Ein，第二部分就是要最小化的目标。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost11.jpg" alt=""></li>
</ul>
</blockquote>
<p><a href=""></a><br>我们对<code>∑un(t)·yn·h(xn)</code>整理一下，对于二元分类情形，我们把yn和h(xn)是否同号进行分别讨论：<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost12.jpg" alt=""><br>上面的公式中，我们特意将<code>∑un(t)·yn·h(xn)</code>拆成<code>-∑un(t)</code>和<code>Ein(h)</code>的形式，这里最小化Ein对应于AdaBoost中的A（弱学习算法），好的弱学习算法就是对应于梯度下降的函数方向。</p>
<p><a href=""></a></p>
<h3 id="2-4-最佳化步长η"><a href="#2-4-最佳化步长η" class="headerlink" title="2.4 最佳化步长η"></a>2.4 最佳化步长η</h3><p>我们要最小化<code>Eada</code>，需要找到好的函数方向gt，但是得打这个gt的代价有些大，梯度下降的过程中，每走一小步，就需要计算得到一个gt。如果转换一下思路，我们现在已经确定了好的gt，我们希望快速找到梯度下降的最低点，那么我们需要找到一个合适的最大步长η。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost13.jpg" alt=""></p>
<p>我们这里使用贪心算法来得到最大步长η，称为steepest decent for optimization。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost14.jpg" alt=""><br>让Eada对η求偏微分，得到最陡时候的ηt，我们发现这时ηt等于AdaBoost的αt。所以在AdaBoost中αt是在偷偷地做最佳化的工作。</p>
<h3 id="2-5-小结"><a href="#2-5-小结" class="headerlink" title="2.5 小结"></a>2.5 小结</h3><p>在第二小节中，我们从另外一个角度介绍了AdaBoost算法，它其实是steepest gradient decent。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/adaboost15.jpg" alt=""><br>上面的式子很清楚了，<strong>我们将AdaBoost的误差函数看做是指数误差函数，AdaBoost就是在这个函数上一步一步做最佳化，每一步求得一个h，并将该h当做是gt，决定这个gt上面要走多长的距离ηt，最终得到这个gt的票数αt。</strong></p>
<h2 id="3-梯度提升-Gradient-Boosting"><a href="#3-梯度提升-Gradient-Boosting" class="headerlink" title="3. 梯度提升(Gradient Boosting)"></a>3. 梯度提升(Gradient Boosting)</h2><p>梯度提升是对AdaBoost的延伸，它不再要求误差函数是指数误差函数，而可能是任意一种误差函数（因为这里是用梯度下降法来最佳化误差函数，所以这里要求误差函数是平滑的）。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/gradient_boost1.jpg" alt=""><br>在这个架构下，我们就可以使用不同的假设和模型，来解决分类或者回归的问题。</p>
<h3 id="3-1-梯度提升应用于回归问题"><a href="#3-1-梯度提升应用于回归问题" class="headerlink" title="3.1 梯度提升应用于回归问题"></a>3.1 梯度提升应用于回归问题</h3><p>梯度提升应用于回归问题时，误差函数选中均方误差函数。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/gradient_boost2.jpg" alt=""></p>
<p>紧接着，我们对这个误差函数中变量s在sn处进行一阶泰勒展开的近似，我们发现要最小化的实际是<code>∑h(xn)·2(sn-yn)</code>，要让该表达式最小，需要<code>h(xn)</code>和<code>(sn-yn)</code>的方向相反：<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/gradient_boost3.jpg" alt=""></p>
<p>要求解最佳化问题，需要<code>h(xn)</code>和<code>(sn-yn)</code>的方向相反，而<code>h(xn)</code>的大小其实不是我们关系的问题，因为步长问题是由参数η决定的。<br>如果将<code>h(xn)</code>强制限制为1或者某个常数的话，那么就得到了一个有条件的最佳化问题，增加了求解的难度。不如我们将惩罚项<code>h(xn)</code>的平方放进最佳化式子中（意思是，如果h(xn)越大，我们就越不希望如此）。<br>我们可以将平方式子变换一下，得到有关<code>(h(xn)-(yn-sn))^2</code>的式子，所以我们要求一个带惩罚项的近似函数梯度的问题，就等效于求<code>xn</code>和余数(residual)<code>yn-sn</code>的回归问题。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/gradient_boost4.jpg" alt=""></p>
<p><a href=""></a><br><strong>确定步长η</strong>：<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/gradient_boost5.jpg" alt=""><br>我们现在确定了gt，接着我们要确定步长η，这里我们可以将误差函数写成余数<code>(yn-sn)</code>的形式，这是一个单变量的线性回归问题，其中输入是用gt转换后的数据，输出是余数(residual)。<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/gradient_boost6.jpg" alt=""></p>
<h2 id="4-梯度提升决策树"><a href="#4-梯度提升决策树" class="headerlink" title="4. 梯度提升决策树"></a>4. 梯度提升决策树</h2><p>综合第三小节的步骤，我们就可以得到梯度提升决策树的算法流程：<br><img src="http://7nj1qk.com1.z0.glb.clouddn.com/@/ML/techniques/gbdt/gradient_boost7.jpg" alt=""></p>
<blockquote>
<ul>
<li>在每一次迭代过程，解决一个回归问题，这里可以用CART算法来解{xn, (yn-sn)}的回归问题；</li>
<li>然后，用gt做转换，做一个{gt(xn), yn-sn}的单变量线性回归问题；</li>
<li>更新分数sn；</li>
<li>经过T轮迭代，得到G(x)。<br>这个GBDT算法可以看做是AdaBoost-DTree的回归问题版本。</li>
</ul>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>机器学习技法课程，林轩田，台湾大学<br><a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html" target="_blank" rel="external">决策树模型组合之随机森林与GBDT</a><br><a href="http://blog.csdn.net/yangtrees/article/details/7506052" target="_blank" rel="external">机器学习——Gradient Boost Decision Tree(&amp;Treelink)</a></p>
<p><strong>转载请注明作者Jason Ding及其出处</strong><br><a href="http://jasonding1354.gitcafe.io/" target="_blank" rel="external">GitCafe博客主页(http://jasonding1354.gitcafe.io/)</a><br><a href="http://jasonding1354.github.io/" target="_blank" rel="external">Github博客主页(http://jasonding1354.github.io/)</a><br><a href="http://blog.csdn.net/jasonding1354" target="_blank" rel="external">CSDN博客(http://blog.csdn.net/jasonding1354)</a><br><a href="http://www.jianshu.com/users/2bd9b48f6ea8/latest_articles" target="_blank" rel="external">简书主页(http://www.jianshu.com/users/2bd9b48f6ea8/latest_articles)</a><br><strong>Google搜索jasonding1354进入我的博客主页</strong></p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/Machine-Learning/">Machine Learning</a>
  </div>


<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
</div>



<div class="article-share" id="share">

  
<div class="jiathis_style">
    <span class="jiathis_txt">分享到：</span>
    <a class="jiathis_button_tsina">新浪微博</a>
    <a class="jiathis_button_weixin">微信</a>
    <a class="jiathis_button_twitter">Twitter</a>
    <a class="jiathis_button_evernote">EverNote</a>
    <a href="http://www.jiathis.com/share?uid=1501277" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
</div>
<script type="text/javascript" >
    var jiathis_config={
    data_track_clickback:true,
    sm:"copy,renren,cqq",
    pic:"",
    summary:"",
    
  </script> 
<script type="text/javascript" src="//v3.jiathis.com/code/jia.js?uid=
1394018007141544" charset="utf-8"></script>      


</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2015/07/30/Feature Engineering/【特征工程】特征工程技术与方法/" title="【特征工程】特征工程技术与方法">
  <strong>上一篇</strong><br/>
  <span>
  【特征工程】特征工程技术与方法</span>
</a>
</div>


<div class="next">
<a href="/2015/07/23/Machine Learning/【机器学习基础】随机森林算法/"  title="【机器学习基础】随机森林算法">
 <strong>下一篇</strong><br/> 
 <span>【机器学习基础】随机森林算法
</span>
</a>
</div>

</nav>

	
<section class="comment">
	<div class="ds-thread"></div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#引言"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-加权的决策树算法-Weighted-Decision-Tree-Algorithm"><span class="toc-number">2.</span> <span class="toc-text">1. 加权的决策树算法(Weighted Decision Tree Algorithm)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-弱决策树算法"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 弱决策树算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-AdaBoost-Stump"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 AdaBoost-Stump</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-AdaBoost深入解释和最佳化"><span class="toc-number">3.</span> <span class="toc-text">2. AdaBoost深入解释和最佳化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-投票分数-Voting-Score-和间隔-Margin-的关系"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 投票分数(Voting Score)和间隔(Margin)的关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-AdaBoost误差函数"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 AdaBoost误差函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-AdaBoost误差函数的梯度下降求解"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 AdaBoost误差函数的梯度下降求解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-最佳化步长η"><span class="toc-number">3.4.</span> <span class="toc-text">2.4 最佳化步长η</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-小结"><span class="toc-number">3.5.</span> <span class="toc-text">2.5 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-梯度提升-Gradient-Boosting"><span class="toc-number">4.</span> <span class="toc-text">3. 梯度提升(Gradient Boosting)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-梯度提升应用于回归问题"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 梯度提升应用于回归问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-梯度提升决策树"><span class="toc-number">5.</span> <span class="toc-text">4. 梯度提升决策树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考资料"><span class="toc-number">6.</span> <span class="toc-text">参考资料</span></a></li></ol>
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  
<div class="maps">
    <p class="asidetitle">访问统计</p>
    <br>
    <script type="text/javascript" src="//ra.revolvermaps.com/0/0/6.js?i=0leibswamim&amp;m=0&amp;s=210&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=0" async="async"></script>
</div>


  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
			<li><a href="/categories/Algorithm-Problem/" title="Algorithm Problem">Algorithm Problem<sup>1</sup></a></li>
		
			<li><a href="/categories/C/" title="C++">C++<sup>3</sup></a></li>
		
			<li><a href="/categories/Classic-Algorithm/" title="Classic Algorithm">Classic Algorithm<sup>1</sup></a></li>
		
			<li><a href="/categories/Cluster-Analysis/" title="Cluster Analysis">Cluster Analysis<sup>1</sup></a></li>
		
			<li><a href="/categories/Design-Pattern/" title="Design Pattern">Design Pattern<sup>1</sup></a></li>
		
			<li><a href="/categories/Developer-Kits/" title="Developer Kits">Developer Kits<sup>6</sup></a></li>
		
			<li><a href="/categories/Energy-Big-Data/" title="Energy Big Data">Energy Big Data<sup>2</sup></a></li>
		
			<li><a href="/categories/Feature-Engineering/" title="Feature Engineering">Feature Engineering<sup>3</sup></a></li>
		
			<li><a href="/categories/Functional/" title="Functional">Functional<sup>1</sup></a></li>
		
			<li><a href="/categories/Functional-Programming/" title="Functional Programming">Functional Programming<sup>6</sup></a></li>
		
			<li><a href="/categories/Git/" title="Git">Git<sup>2</sup></a></li>
		
			<li><a href="/categories/Jobs/" title="Jobs">Jobs<sup>2</sup></a></li>
		
			<li><a href="/categories/Linux/" title="Linux">Linux<sup>4</sup></a></li>
		
			<li><a href="/categories/ML-Experiments/" title="ML Experiments">ML Experiments<sup>6</sup></a></li>
		
			<li><a href="/categories/Machine-Learning/" title="Machine Learning">Machine Learning<sup>50</sup></a></li>
		
			<li><a href="/categories/Math/" title="Math">Math<sup>3</sup></a></li>
		
			<li><a href="/categories/Programming/" title="Programming">Programming<sup>11</sup></a></li>
		
			<li><a href="/categories/Python/" title="Python">Python<sup>10</sup></a></li>
		
			<li><a href="/categories/Scala/" title="Scala">Scala<sup>27</sup></a></li>
		
			<li><a href="/categories/Scala-ML/" title="Scala-ML">Scala-ML<sup>2</sup></a></li>
		
			<li><a href="/categories/Similarity-Search/" title="Similarity Search">Similarity Search<sup>4</sup></a></li>
		
			<li><a href="/categories/Small-Problems/" title="Small Problems">Small Problems<sup>1</sup></a></li>
		
			<li><a href="/categories/Spark/" title="Spark">Spark<sup>23</sup></a></li>
		
			<li><a href="/categories/Thoughts/" title="Thoughts">Thoughts<sup>3</sup></a></li>
		
		</ul>
</div>


  
  <div class="tagcloudlist">
    <p class="asidetitle">Tag Cloud</p>
    <div class="tagcloudlist clearfix">
       <a href="/tags/Actor/" style="font-size: 10px;">Actor</a> <a href="/tags/Akka/" style="font-size: 14.55px;">Akka</a> <a href="/tags/Algorithm/" style="font-size: 10px;">Algorithm</a> <a href="/tags/Approximate-Nearest-Neighbor-Search/" style="font-size: 10px;">Approximate Nearest Neighbor Search</a> <a href="/tags/Bayesian-Analysis/" style="font-size: 10.91px;">Bayesian Analysis</a> <a href="/tags/Big-Data/" style="font-size: 10px;">Big Data</a> <a href="/tags/C/" style="font-size: 12.73px;">C++</a> <a href="/tags/Classification/" style="font-size: 11.82px;">Classification</a> <a href="/tags/Cluster-Analysis/" style="font-size: 10px;">Cluster Analysis</a> <a href="/tags/Computer-Vision/" style="font-size: 14.55px;">Computer Vision</a> <a href="/tags/Eclipse/" style="font-size: 10px;">Eclipse</a> <a href="/tags/Energy-Forecasting/" style="font-size: 10px;">Energy Forecasting</a> <a href="/tags/Feature-Engineering/" style="font-size: 11.82px;">Feature Engineering</a> <a href="/tags/Functional-Programming/" style="font-size: 16.36px;">Functional Programming</a> <a href="/tags/Gaussian-Process/" style="font-size: 10px;">Gaussian Process</a> <a href="/tags/Git/" style="font-size: 10.91px;">Git</a> <a href="/tags/GraphLab/" style="font-size: 10px;">GraphLab</a> <a href="/tags/IPython/" style="font-size: 10px;">IPython</a> <a href="/tags/Internet-of-Energy/" style="font-size: 10px;">Internet of Energy</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Kaggle/" style="font-size: 10px;">Kaggle</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/LSH/" style="font-size: 12.73px;">LSH</a> <a href="/tags/LaTex/" style="font-size: 10px;">LaTex</a> <a href="/tags/Linux/" style="font-size: 11.82px;">Linux</a> <a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a> <a href="/tags/Math/" style="font-size: 11.82px;">Math</a> <a href="/tags/Mathematics/" style="font-size: 13.64px;">Mathematics</a> <a href="/tags/Maven/" style="font-size: 10.91px;">Maven</a> <a href="/tags/MeanShift/" style="font-size: 10px;">MeanShift</a> <a href="/tags/Mint/" style="font-size: 10px;">Mint</a> <a href="/tags/OpenCV/" style="font-size: 15.45px;">OpenCV</a> <a href="/tags/Probabilistic-Programming/" style="font-size: 10px;">Probabilistic Programming</a> <a href="/tags/Probability-Distributions/" style="font-size: 10px;">Probability Distributions</a> <a href="/tags/Programming/" style="font-size: 14.55px;">Programming</a> <a href="/tags/Python/" style="font-size: 17.27px;">Python</a> <a href="/tags/Regression/" style="font-size: 11.82px;">Regression</a> <a href="/tags/Resume/" style="font-size: 10px;">Resume</a> <a href="/tags/SVM/" style="font-size: 12.73px;">SVM</a> <a href="/tags/Scala/" style="font-size: 19.09px;">Scala</a> <a href="/tags/Similarity-Search/" style="font-size: 10.91px;">Similarity Search</a> <a href="/tags/Source-code/" style="font-size: 10px;">Source code</a> <a href="/tags/Spark/" style="font-size: 18.18px;">Spark</a> <a href="/tags/Summary/" style="font-size: 10px;">Summary</a> <a href="/tags/Thoughts/" style="font-size: 10.91px;">Thoughts</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Vim/" style="font-size: 11.82px;">Vim</a> <a href="/tags/Written-Test/" style="font-size: 10px;">Written Test</a> <a href="/tags/Zeppelin/" style="font-size: 10px;">Zeppelin</a> <a href="/tags/boost/" style="font-size: 10px;">boost</a> <a href="/tags/cpp/" style="font-size: 13.64px;">cpp</a> <a href="/tags/opencv/" style="font-size: 10.91px;">opencv</a> <a href="/tags/p-Stable-LSH/" style="font-size: 10px;">p-Stable LSH</a> <a href="/tags/scikit-learn/" style="font-size: 10px;">scikit-learn</a> <a href="/tags/sklearn/" style="font-size: 10.91px;">sklearn</a> <a href="/tags/互联网时代/" style="font-size: 10px;">互联网时代</a> <a href="/tags/位置敏感哈希/" style="font-size: 10.91px;">位置敏感哈希</a> <a href="/tags/分治/" style="font-size: 10px;">分治</a> <a href="/tags/单元测试/" style="font-size: 10px;">单元测试</a> <a href="/tags/反思/" style="font-size: 10px;">反思</a> <a href="/tags/效率/" style="font-size: 10px;">效率</a> <a href="/tags/核方法/" style="font-size: 10px;">核方法</a> <a href="/tags/算法框架/" style="font-size: 10px;">算法框架</a> <a href="/tags/设计模式/" style="font-size: 11.82px;">设计模式</a>
    </div>
  </div>


  
  <div class="archiveslist">
    <p class="asidetitle"><a href="/archives">Archives</a></p>
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">29</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">16</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">December 2014</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">November 2014</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/10/">October 2014</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/09/">September 2014</a><span class="archive-list-count">10</span></li></ul>
  </div>


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

</aside>

</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Jason Ding in HUST <br/>
			I would share moments of life here! </p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/1698420390" target="_blank" title="weibo"></a>
		
		
		
		<a href="https://github.com/jasonding1354" target="_blank" title="github"></a>
		
		
	</div>
		<p class="copyright">Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/A-limon/pacman" target="_blank" title="Pacman">Pacman</a> © 2017 
		
		<a href="http://blog.jasonding.top" target="_blank" title="Jason Ding">Jason Ding</a>
		
		</p>
		<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');
  
  _st('install','uYyEuxJdzzAGXTax33mt','2.0.0');
</script>
</div>
</footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{
    c.click(function(){
      ta.css('display', 'block').addClass('fadeIn');
    });
    o.click(function(){
      ta.css('display', 'none');
    });
    $(window).scroll(function(){
      ta.css("top",Math.max(140,320-$(this).scrollTop()));
    });
  };
});
</script>



<script type="text/javascript">
  var duoshuoQuery = {short_name:"jasonding"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 





  </body>
</html>
